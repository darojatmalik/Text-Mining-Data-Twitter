library(cluster)
kmeansResult
tdms
write.csv(tdms, file = "tdms.csv")
tdms
abcd <- data.frame(tdms)
sse
print(tdms)
print(sse)
getwd()
tdms
ptm <- proc.time()
library(twitteR)
library(tm)
library(NLP)
library(stringr)
source('stemm.R')
source('singkatan.R')
library(fpc)
library(base)
library(stats)
library(dplyr)
library(dplyr)
library(fpc)
library(cluster)
tdm
tdms
View(tdms)
saveRDS(tdms, file="data/tdms.rds")
write.csv(tdms, file = "data/tdms.csv")
m1
dim(m1)
dim(m2)
dim(m1)
write.csv(dim(m1), file = "data/tdms.csv")
write.csv(dim(m1), file = "data_praproses/tdms.csv")
load("D:/kuliah/SKRIPSI/leaflet/tutorial R shiny/latihan4/.RData")
shiny::runApp('D:/kuliah/SKRIPSI/leaflet/tutorial R shiny/latihan4')
ptm <- proc.time()
library(twitteR)
library(tm)
library(NLP)
library(stringr)
source('stemm.R')
source('singkatan.R')
library(fpc)
library(base)
library(stats)
library(dplyr)
library(dplyr)
library(fpc)
library(cluster)
baru1
names(baru1)[names(baru1)=="hasilnormalbaru"] <- "text"
View(baru1)
View(baru1)
write.csv(names(baru1)[names(baru1)=="hasilnormalbaru"] <- "text", file = "data_praprocess/hasilpraproses.csv")
baru1
write.csv(baru1, file="data_praproses/baru1.csv")
ptm <- proc.time()
library(twitteR)
library(tm)
library(NLP)
library(stringr)
source('stemm.R')
source('singkatan.R')
library(fpc)
library(base)
library(stats)
library(dplyr)
library(dplyr)
library(fpc)
library(cluster)
datatwitter <- read.csv(file = "datafinal2.csv", head = TRUE)
korpus <- Corpus(VectorSource(datatwitter$text))
#Tokenisasi
hapusURL <- function(x) gsub("(http)[[:graph:]]*", "", x)
korpus <- tm_map(korpus, content_transformer(hapusURL))
enter <- function(x) gsub("\n", " ", x)
korpus <- tm_map(korpus, content_transformer(enter))
karakter1 <- function(x) gsub("\\S*(\\S)\\1\\1\\S*\\s?", "", x)
korpus <- tm_map(korpus, content_transformer(karakter1))
karakter2 <- function(x) gsub("(@)[[:graph:]]+", "", x)
korpus <- tm_map(korpus, content_transformer(karakter2))
#karakter3 <- function(x) gsub("(#)[[:graph:]]+", "", x)
#korpus <- tm_map(korpus, content_transformer(karakter3))
karakter4 <- function(x) gsub("&lt;-"," ", x)
korpus <- tm_map(korpus, content_transformer(karakter4))
karakter5 <- function(x) gsub("[[:punct:]]", " ", x)
korpus <- tm_map(korpus, content_transformer(karakter5))
karakter6 <- function(x) gsub("[[:digit:]]", " ", x)
korpus <- tm_map(korpus, content_transformer(karakter6))
#membuat korpus dengan library TM
korpus <- tm_map(korpus, content_transformer(tolower))
korpus <- tm_map(korpus, stripWhitespace)
korpus <- tm_map(korpus, removeNumbers)
korpus <- tm_map(korpus, removePunctuation)
korpus <- tm_map(korpus, stripWhitespace)
#membuat data frame untuk normalisasi
dataframe<-data.frame(text=unlist(sapply(korpus, "content")))
splitgue<-strsplit(as.character(dataframe$text), split = " ")
unlistgue<-unlist(splitgue)
data1 <- data.frame(unlistgue)
#normalisasi dengan mengubah singkatan
nbaris <- length(splitgue)
for (i in 1:nbaris) {
nkolom <- length(splitgue[i][[1]])
if(nkolom != 0)
{
nkolom <- length(splitgue[i][[1]])
for (j in 1:nkolom) {
term <- splitgue[i][[1]][[j]]
norm <- cari.singkatan(term)
splitgue[i][[1]][[j]] <- norm
}
}
}
ptm <- proc.time()
library(twitteR)
library(tm)
library(NLP)
library(stringr)
source('stemm.R')
source('singkatan.R')
library(fpc)
library(base)
library(stats)
library(dplyr)
library(dplyr)
library(fpc)
library(cluster)
datatwitter <- read.csv(file = "datafinal2.csv", head = TRUE)
korpus <- Corpus(VectorSource(datatwitter$text))
#Tokenisasi
hapusURL <- function(x) gsub("(http)[[:graph:]]*", "", x)
korpus <- tm_map(korpus, content_transformer(hapusURL))
enter <- function(x) gsub("\n", " ", x)
korpus <- tm_map(korpus, content_transformer(enter))
karakter1 <- function(x) gsub("\\S*(\\S)\\1\\1\\S*\\s?", "", x)
korpus <- tm_map(korpus, content_transformer(karakter1))
karakter2 <- function(x) gsub("(@)[[:graph:]]+", "", x)
korpus <- tm_map(korpus, content_transformer(karakter2))
#karakter3 <- function(x) gsub("(#)[[:graph:]]+", "", x)
#korpus <- tm_map(korpus, content_transformer(karakter3))
karakter4 <- function(x) gsub("&lt;-"," ", x)
korpus <- tm_map(korpus, content_transformer(karakter4))
karakter5 <- function(x) gsub("[[:punct:]]", " ", x)
korpus <- tm_map(korpus, content_transformer(karakter5))
karakter6 <- function(x) gsub("[[:digit:]]", " ", x)
korpus <- tm_map(korpus, content_transformer(karakter6))
#membuat korpus dengan library TM
korpus <- tm_map(korpus, content_transformer(tolower))
korpus <- tm_map(korpus, stripWhitespace)
korpus <- tm_map(korpus, removeNumbers)
korpus <- tm_map(korpus, removePunctuation)
korpus <- tm_map(korpus, stripWhitespace)
#membuat data frame untuk normalisasi
dataframe<-data.frame(text=unlist(sapply(korpus, "content")))
splitgue<-strsplit(as.character(dataframe$text), split = " ")
unlistgue<-unlist(splitgue)
data1 <- data.frame(unlistgue)
#normalisasi dengan mengubah singkatan
nbaris <- length(splitgue)
for (i in 1:nbaris) {
nkolom <- length(splitgue[i][[1]])
if(nkolom != 0)
{
nkolom <- length(splitgue[i][[1]])
for (j in 1:nkolom) {
term <- splitgue[i][[1]][[j]]
norm <- cari.singkatan(term)
splitgue[i][[1]][[j]] <- norm
}
}
}
Length <- sapply(splitgue, length)
max.length <-max(sapply(splitgue,length))
#hasil normalisasi
hasilnormal<-lapply(splitgue,function(v) {c (v, rep("",max.length-length(v)))})
hasilnormal<-do.call(rbind,hasilnormal)
hasilnormal<-data.frame(hasilnormal)
#menghapus stopwords
dataframe1 <-data.frame(text=unlist(sapply(korpus, "content")))
newdat <- data.frame(text=str_trim(do.call(paste, hasilnormal)), stringsAsFactors=FALSE)
baru<-data.frame(newdat)
names(baru)[names(baru)=="hasilnormalbaru"] <- "text"
korpus1 <- Corpus(VectorSource(baru$text))
korpus1 <- tm_map(korpus1, stripWhitespace)
stopword<- read.csv("stopword.csv")
stopword <- tolower(stopword[, 1])
stopword <- c(stopword, "", "a", "-", "rt", "...", "&", "|", "ada")
#stopswords
korpus1=tm_map(korpus1,removeWords,stopword)
korpus1 <- tm_map(korpus1, stripWhitespace)
korpus1 <- tm_map(korpus1, removeNumbers)
korpus1 <- tm_map(korpus1, removePunctuation)
korpus1 <- tm_map(korpus1, content_transformer(tolower))
korpus1 <- tm_map(korpus1, stripWhitespace)
#membuat data frame untuk stemming
dataframe2<-data.frame(text=unlist(sapply(korpus1, "content")))
splitgue1<-strsplit(as.character(dataframe2$text), split = " ")
unlistgue1<-unlist(splitgue1)
data2 <- data.frame(unlistgue1)
#Stemming
nbaris1 <- length(splitgue1)
for (m in 1:nbaris1) {
nkolom1 <- length(splitgue1[m][[1]])
if(nkolom1 != 0)
{
for (n in 1:nkolom1) {
#print(n)
term_stem <- splitgue1[m][[1]][[n]]
#print(term_stem)
norm_stem <- stemming(term_stem)
#print(norm_stem)
splitgue1[m][[1]][[n]] <- norm_stem
}
}
}
Length <- sapply(splitgue1, length)
max.length <-max(sapply(splitgue1,length))
#hasil stemming
hasilnormal1<-lapply(splitgue1,function(v) {c (v, rep("",max.length-length(v)))})
hasilnormal1<-do.call(rbind,hasilnormal1)
hasilnormal1<-data.frame(hasilnormal1)
#pembuatan dataframe untuk Term Document Matrix
library(stringr)
newdat1 <- data.frame(text=str_trim(do.call(paste, hasilnormal1)), stringsAsFactors=FALSE)
baru1<-data.frame(newdat1)
names(baru1)[names(baru1)=="hasilnormalbaru"] <- "text"
#TDM
baru1
v <- Corpus(VectorSource(baru1$text))
tdm <- TermDocumentMatrix(v)
tdm
#tdm to matrix
m <- as.matrix(tdm)
dim(m)
#mereduksi terms dengan remove sparse term
tdms <- removeSparseTerms(tdm, 0.99)
inspect(tdms)
m1 <- as.matrix(tdms)
dim(m1)
tdms
#Clustering dengan K-Means
library(fpc)
#Tranpose Matrix m1 ke m2
m2 <- t(m1)
#SSE
s <- 2
finish <- 7
for(start in s : finish){
set.seed(1000)
hasil <- kmeans(m2, start)
centers <- hasil$centers[hasil$cluster,,drop=FALSE]
jarak <- sqrt((m2 - centers)^2)
total <- sum(jarak)
persen <- 100*(hasil$betweenss/hasil$totss)
gab<- data.frame(start, total, hasil$tot.withinss, persen)
if(start == 2) {
sse <- gab
sse <- as.matrix(sse)
}
else{
sse <- rbind(sse, gab)
}
}
names(sse) <- c("kelas", "sse", "tot.within", "persen")
# set a fixed random seed
set.seed(122)
# k-means clustering of tweets
k <-7
kmeansResult <- kmeans(m2, k)
# cluster centers
round(kmeansResult$centers, digits=3)
#cetak hasil clustering
write.csv(kmeansResult$cluster, file="hasil_cluster.csv")
#marge data untuk hasil data teks hasil praproses, cluster pada data Twitter
korpus_username <- Corpus(VectorSource(datatwitter$username))
df_username<-data.frame(username=unlist(sapply(korpus_username, "content")))
korpus_date <- Corpus(VectorSource(datatwitter$date))
df_date<-data.frame(date=unlist(sapply(korpus_date, "content")))
korpus_time <- Corpus(VectorSource(datatwitter$time))
df_time<-data.frame(time=unlist(sapply(korpus_time, "content")))
korpus_tweetID <- Corpus(VectorSource(datatwitter$tweetID))
df_tweetID<-data.frame(tweetID=unlist(sapply(korpus_tweetID, "content")))
korpus_placeID <- Corpus(VectorSource(datatwitter$placeID))
df_placeID<-data.frame(placeID=unlist(sapply(korpus_placeID, "content")))
korpus_pusatkota <- Corpus(VectorSource(datatwitter$pusatkota))
df_pusatkota<-data.frame(pusatkota=unlist(sapply(korpus_pusatkota, "content")))
korpus_lokasi <- Corpus(VectorSource(datatwitter$lokasi))
df_lokasi<-data.frame(lokasi=unlist(sapply(korpus_lokasi, "content")))
korpus_longitude <- Corpus(VectorSource(datatwitter$longitude))
df_longitude<-data.frame(longitude=unlist(sapply(korpus_longitude, "content")))
korpus_latitude <- Corpus(VectorSource(datatwitter$latitude))
df_latitude<-data.frame(latitude=unlist(sapply(korpus_latitude, "content")))
hasil_cluster <- read.csv(file = "hasil_cluster.csv", head = TRUE)
korpus_cluster <- Corpus(VectorSource(hasil_cluster$x))
df_cluster<-data.frame(cluster=unlist(sapply(korpus_cluster, "content")))
df_total<-cbind(df_username,df_date, df_time, newdat1,df_tweetID, df_placeID, df_pusatkota, df_lokasi, df_longitude, df_latitude, df_cluster)
write.csv(df_total, file="temp/hasil_praproses_clustering.csv")
library(cluster)
library(factoextra)
library(ggplot2)
#Membuat visualisasi Plot
clusplot(m2, kmeansResult$cluster, colot = TRUE, shade = TRUE, labels = 2, lines = 0)
fviz_cluster(kmeansResult, data = m2)
# Membuat WordCloud
# library(wordcloud)
# #m1 <- as.matrix(tdms)
# #calculate the frequency of words and sort it descendingly by frequency
# wordFreq <- sort(rowSums(m1), decreasing = TRUE)
# #word cloud
# set.seed(375) #to make it reproducible
# grayLevels <- gray((wordFreq + 10) / (max(wordFreq) + 10))
# wordcloud(words = names(wordFreq), freq=wordFreq, min.freq=3, random.order = F, colors = grayLevels)
hasil
kmeansResult$cluster
kmeansResult$size
shiny::runApp('D:/kuliah/SKRIPSI/leaflet/tutorial R shiny/latihan5')
shiny::runApp()
runApp()
shiny::runApp()
getwd()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp('D:/kuliah/SKRIPSI/leaflet/tutorial R shiny/latihan6 edited database')
runApp()
shiny::runApp()
shiny::runApp()
runApp()
runApp()
View(datatwitter)
View(datatwitter)
shiny::runApp()
runApp()
getwd()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
tweet <- readRDS("temp/datatwitter2.rds")
tweet$longitude <- jitter(tweet$longitude)
tweet
View(tweet)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
